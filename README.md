# Privacy-Leakage-Detection

This project provides a privacy leakage detection framework that evaluates whether contextual visual and textual content may lead to unintended privacy disclosure. It includes a unified main framework combining LLM and VLM, as well as LLM-only and VLM-only baselines for comparison. The detection is performed over image-context pairs under both privacy and non-privacy conditions.

---

## ğŸ“ Project Structure

```bash
PRIVACY-LEAKAGE-DETECTION/
â”‚
â”œâ”€â”€ datasets/                          # Input data (images and context)
â”‚   â”œâ”€â”€ input_images/                  # Raw image files
â”‚   â”œâ”€â”€ context_privacy.json           # Contexts containing privacy information
â”‚   â””â”€â”€ context_nonprivacy.json        # Contexts without privacy risk
â”‚
â”œâ”€â”€ llm_baseline/                      # LLM-only detection pipeline
â”‚   
â”œâ”€â”€ vlm_baseline/                      # VLM-only detection pipeline
â”‚   
â”œâ”€â”€ main_framework/                    # Full pipeline using both LLM and VLM
â”‚
â””â”€â”€ README.md                          # Project documentation (this file)

## ğŸ“¦ Acknowledgement

The `main_framework` component of this project used the open-source repository [Recognize Anything](https://github.com/xinyu1205/recognize-anything) by [@xinyu1205](https://github.com/xinyu1205).  
We gratefully acknowledge their contributions and reuse of their work in our privacy leakage detection pipeline.

Please refer to the original repository for full implementation details and license terms.
